# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1igUHyKMull_0BW063qztaDUgn8qZ2CkB
"""

# Install required packages first:
# !pip install llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface
# !pip install transformers accelerate bitsandbytes pypdf2
# !pip install fastapi uvicorn pyngrok nest-asyncio
# !pip install -U bitsandbytes

# Install required packages first:
!pip install llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface
!pip install transformers accelerate bitsandbytes pypdf2
!pip install fastapi uvicorn pyngrok nest-asyncio
!pip install -U bitsandbytes

import os
import time
import torch
import nest_asyncio
from typing import Optional
from PyPDF2 import PdfReader
from pydantic import BaseModel
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.responses import JSONResponse
from llama_index.core import VectorStoreIndex, Settings, PromptTemplate
from llama_index.core.schema import Document
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from transformers import BitsAndBytesConfig
import uvicorn
from pyngrok import ngrok

# Enable nested event loops for Colab
nest_asyncio.apply()

# ============================================================
# 1. Configuration
# ============================================================
PDF_PATH = "/content/drive/MyDrive/Gen AI Mini Projects/RAG Chatbot/data/Vishal-Yadav-30-Dec-2025.pdf"
CHUNK_SIZE = 512
CHUNK_OVERLAP = 50
TOP_K = 3

# ============================================================
# 2. Global Variables
# ============================================================
query_engine = None
app = FastAPI(title="RAG Chatbot API", version="1.0.0")

# ============================================================
# 3. Pydantic Models
# ============================================================
class QueryRequest(BaseModel):
    question: str
    top_k: Optional[int] = TOP_K

class QueryResponse(BaseModel):
    answer: str
    processing_time: float

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool

# ============================================================
# 4. Helper Functions
# ============================================================
def extract_text_from_pdf(pdf_path):
    """Extract text from PDF file"""
    print("üìÑ Extracting PDF...")
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PdfReader(file)
        for page in reader.pages:
            content = page.extract_text()
            if content:
                text += content
    return text

def initialize_rag_system(pdf_path=PDF_PATH):
    """Initialize the entire RAG pipeline"""
    global query_engine

    print("üöÄ Initializing RAG System...")

    # Extract PDF text
    resume_text = extract_text_from_pdf(pdf_path)
    documents = [Document(text=resume_text)]

    # Setup Embedding Model
    print("üî§ Loading Embedding Model...")
    Settings.embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # Setup LLM with 4-bit quantization
    print("ü§ñ Loading Mistral-7B (4-bit)...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

    model_id = "mistralai/Mistral-7B-Instruct-v0.3"

    llm = HuggingFaceLLM(
        model_name=model_id,
        tokenizer_name=model_id,
        context_window=4096,
        max_new_tokens=512,
        generate_kwargs={"temperature": 0.1, "do_sample": False},
        model_kwargs={"quantization_config": quantization_config},
        device_map="auto",
    )

    Settings.llm = llm

    # Create Vector Index
    print("üíæ Creating Vector Index...")
    index = VectorStoreIndex.from_documents(
        documents,
        transformations=[Settings.embed_model]
    )

    # Custom Prompt Template
    qa_prompt_tmpl = (
        "<s>[INST] Context information is below.\n"
        "---------------------\n"
        "{context_str}\n"
        "---------------------\n"
        "Given the context information and not prior knowledge, "
        "answer the query.\n"
        "Query: {query_str}\n"
        "Answer: [/INST]"
    )
    qa_prompt = PromptTemplate(qa_prompt_tmpl)

    # Create Query Engine
    query_engine = index.as_query_engine(
        similarity_top_k=TOP_K,
        text_qa_template=qa_prompt,
        response_mode="compact"
    )

    print("‚úÖ RAG System Initialized Successfully!")
    return query_engine

# ============================================================
# 5. FastAPI Endpoints
# ============================================================
@app.on_event("startup")
async def startup_event():
    """Initialize RAG system on startup"""
    initialize_rag_system()

@app.get("/", response_model=dict)
async def root():
    """Root endpoint"""
    return {
        "message": "RAG Chatbot API is running!",
        "endpoints": {
            "/health": "Check API health",
            "/query": "Query the RAG system (POST)",
            "/docs": "Interactive API documentation"
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    return HealthResponse(
        status="healthy" if query_engine is not None else "initializing",
        model_loaded=query_engine is not None
    )

@app.post("/query", response_model=QueryResponse)
async def query_rag(request: QueryRequest):
    """
    Query the RAG system with a question

    Example:
    {
        "question": "What are the skills mentioned in the resume?",
        "top_k": 3
    }
    """
    if query_engine is None:
        raise HTTPException(status_code=503, detail="RAG system not initialized")

    if not request.question.strip():
        raise HTTPException(status_code=400, detail="Question cannot be empty")

    try:
        start_time = time.time()

        # Update top_k if provided
        if request.top_k != TOP_K:
            query_engine.update_prompts(similarity_top_k=request.top_k)

        # Query the system
        response = query_engine.query(request.question)
        processing_time = time.time() - start_time

        return QueryResponse(
            answer=str(response),
            processing_time=round(processing_time, 2)
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")

@app.post("/reinitialize")
async def reinitialize(pdf_path: Optional[str] = None):
    """Reinitialize the RAG system with a new PDF"""
    try:
        path = pdf_path if pdf_path else PDF_PATH
        initialize_rag_system(path)
        return {"message": "RAG system reinitialized successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reinitializing: {str(e)}")

# ============================================================
# 6. Run Server with Ngrok (for Colab)
# ============================================================
async def start_server(port: int = 8000):
    """Async function to start the uvicorn server"""
    from uvicorn import Config, Server

    config = Config(app=app, host="0.0.0.0", port=port, log_level="info")
    server = Server(config)
    await server.serve()

def run_server(ngrok_token: Optional[str] = None, port: int = 8000):
    """
    Run the FastAPI server with ngrok tunneling

    Args:
        ngrok_token: Your ngrok auth token (get it from https://dashboard.ngrok.com/get-started/your-authtoken)
        port: Port to run the server on
    """
    import asyncio
    import threading

    # Set ngrok auth token if provided
    if ngrok_token:
        ngrok.set_auth_token(ngrok_token)

    # Start the server in a background thread
    def run_async_server():
        asyncio.run(start_server(port))

    server_thread = threading.Thread(target=run_async_server, daemon=True)
    server_thread.start()

    # Wait a moment for server to start
    time.sleep(3)

    # Start ngrok tunnel
    print(f"üåê Starting ngrok tunnel on port {port}...")
    public_url = ngrok.connect(port)

    print(f"\n{'='*60}")
    print(f"üöÄ FastAPI server is running!")
    print(f"{'='*60}")
    print(f"üì° Public URL: {public_url}")
    print(f"üìö API Docs: {public_url}/docs")
    print(f"üìù Interactive Docs: {public_url}/redoc")
    print(f"{'='*60}\n")
    print("‚úÖ Server is ready to accept requests!")
    print("‚ö†Ô∏è  Keep this cell running - don't interrupt it!")
    print(f"{'='*60}\n")

    # Keep the main thread alive
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nüõë Server stopped by user")

# ============================================================
# 8. Quick Start (Uncomment to run)
# ============================================================
if __name__ == "__main__":
    # IMPORTANT: Add your ngrok token here
    NGROK_TOKEN = "37kUjAr0yhRR5pqAFklMFkK5NcF_6xooTD3a6s1xiqofVHDu6"  # Get from https://dashboard.ngrok.com

    # Mount Google Drive first if needed
    # from google.colab import drive
    # drive.mount('/content/drive')

    # Run the server
    run_server(ngrok_token=NGROK_TOKEN, port=8000)



